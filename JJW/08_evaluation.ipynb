{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wjs31\\anaconda3\\envs\\Torch_NLP\\lib\\site-packages\\pytorch_forecasting\\models\\base_model.py:30: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import MAE, SMAPE, PoissonLoss, QuantileLoss\n",
    "# from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = 'lightning_logs\\\\lightning_logs\\\\version_0\\\\checkpoints\\\\epoch=29-step=510.ckpt'\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "data = pd.read_csv('../DATA/data_with_btc_scaled.csv')\n",
    "# data.head(), data.info(), data.describe()\n",
    "data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\n",
    "\n",
    "# add time index : datetime to int\n",
    "data[\"time_idx\"] = (data[\"date\"].dt.year * 12 + data[\"date\"].dt.month) * 30 + data[\"date\"].dt.day\n",
    "data[\"time_idx\"] -= data[\"time_idx\"].min()\n",
    "data['time_idx'] = data['time_idx'].astype(int)\n",
    "# 여기서 day 반영\n",
    "# data[\"time_idx\"] += data[\"date\"].dt.day / 30.5\n",
    "\n",
    "# add additional features\n",
    "data[\"month\"] = data.date.dt.month.astype(str).astype\n",
    "data['group'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_encoder_length = 300 \n",
    "max_prediction_length = 30 \n",
    "training_cutoff = data[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"btc\",\n",
    "    group_ids=[\"group\"],\n",
    "    min_encoder_length=max_encoder_length // 2,  # keep encoder length long (as it is in the validation set)\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    # static_categoricals=[\"agency\", \"sku\"],\n",
    "    # static_reals=[\"avg_population_2017\", \"avg_yearly_household_income_2017\"],\n",
    "    time_varying_known_categoricals=[\"month\"],\n",
    "    # variable_groups={\"special_days\": special_days},  # group of categorical variables can be treated as one variable\n",
    "    time_varying_known_reals=[\"time_idx\"],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\"Gold\", \"Dollar\", \"SnP\"],\n",
    "    # target_normalizer=GroupNormalizer(\n",
    "        # groups=[\"agency\", \"sku\"], transformation=\"softplus\"\n",
    "    # ),  # use softplus and normalize by group\n",
    "    allow_missing_timesteps=True,\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")\n",
    "\n",
    "# create validation set (predict=True) which means to predict the last max_prediction_length points in time\n",
    "# for each series\n",
    "validation = TimeSeriesDataSet.from_dataset(training, data, predict=True, stop_randomization=True)\n",
    "\n",
    "# create dataloaders for model\n",
    "batch_size = 128  # set this between 32 to 128\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# calcualte mean absolute error on validation set\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m best_tft\u001b[38;5;241m.\u001b[39mpredict(\u001b[43mval_dataloader\u001b[49m, return_y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, trainer_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      3\u001b[0m MAE()(predictions\u001b[38;5;241m.\u001b[39moutput, predictions\u001b[38;5;241m.\u001b[39my)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'val_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "# calcualte mean absolute error on validation set\n",
    "predictions = best_tft.predict(val_dataloader, return_y=True, trainer_kwargs=dict(accelerator=\"cpu\"))\n",
    "MAE()(predictions.output, predictions.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw predictions are a dictionary from which all kind of information including quantiles can be extracted\n",
    "raw_predictions = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(1):  # plot 10 examples\n",
    "    best_tft.plot_prediction(raw_predictions.x, raw_predictions.output, idx=idx, add_loss_to_title=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcualte metric by which to display\n",
    "predictions = best_tft.predict(val_dataloader, return_y=True)\n",
    "mean_losses = SMAPE(reduction=\"none\")(predictions.output, predictions.y).mean(1)\n",
    "indices = mean_losses.argsort(descending=True)  # sort losses\n",
    "for idx in range(1):  # plot 10 examples\n",
    "    best_tft.plot_prediction(\n",
    "        raw_predictions.x,\n",
    "        raw_predictions.output,\n",
    "        idx=indices[idx],\n",
    "        add_loss_to_title=SMAPE(quantiles=best_tft.loss.quantiles),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch_NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
