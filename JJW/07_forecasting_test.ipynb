{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Forecasting 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch 사용해보기\n",
    "# %pip install pytorch-forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo\n",
    "# 1. Create a training dataset -> TimeSeriesDataSet\n",
    "# 2. Create a validation dataset -> from_dataset()\n",
    "# 3. Instantiate a trainer -> .from_dataset()\n",
    "# 4. Create a lighting.Trainer() object\n",
    "# 5. Find Optimal Learning Rate -> .tuner.lr_find()\n",
    "# 6. Train the model -> .fit()\n",
    "# 7. Make predictions -> .predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example from homepage of Pytorch Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
    "\n",
    "# load data\n",
    "data = ...\n",
    "\n",
    "# define dataset\n",
    "max_encoder_length = 36                 # length of input\n",
    "max_prediction_length = 6               # length of output\n",
    "training_cutoff = \"YYYY-MM-DD\"          # day for cutoff\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.date < training_cutoff],   # filter data\n",
    "    time_idx= ...,                              # time index\n",
    "    target= ...,                                # target column\n",
    "    # weight=\"weight\",\n",
    "    group_ids=[ ... ],                          # group id\n",
    "    max_encoder_length=max_encoder_length,      # length of input\n",
    "    max_prediction_length=max_prediction_length,    # length of output\n",
    "    static_categoricals=[ ... ],                    # static categorical columns\n",
    "    static_reals=[ ... ],                           # static real columns\n",
    "    time_varying_known_categoricals=[ ... ],    # time varying known categorical columns\n",
    "    time_varying_known_reals=[ ... ],           # time varying known real columns\n",
    "    time_varying_unknown_categoricals=[ ... ],  # time varying unknown categorical columns\n",
    "    time_varying_unknown_reals=[ ... ],         # time varying unknown real columns\n",
    ")\n",
    "\n",
    "# create validation and training dataset\n",
    "validation = TimeSeriesDataSet.from_dataset(training, data, min_prediction_idx=training.index.time.max() + 1, stop_randomization=True)\n",
    "batch_size = 128\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=2)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=2)\n",
    "\n",
    "# define trainer with early stopping\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=1, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    accelerator=\"auto\",\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=30,\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    ")\n",
    "\n",
    "# create the model\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=32,\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=16,\n",
    "    output_size=7,\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=2,\n",
    "    reduce_on_plateau_patience=4\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n",
    "\n",
    "# find optimal learning rate (set limit_train_batches to 1.0 and log_interval = -1)\n",
    "res = Tuner(trainer).lr_find(\n",
    "    tft, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader, early_stop_threshold=1000.0, max_lr=0.3,\n",
    ")\n",
    "\n",
    "print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "fig = res.plot(show=True, suggest=True)\n",
    "fig.show()\n",
    "\n",
    "# fit the model\n",
    "trainer.fit(\n",
    "    tft, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Github Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "import torch\n",
    "\n",
    "from pytorch_forecasting import GroupNormalizer, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data.examples import get_stallion_data\n",
    "from pytorch_forecasting.metrics import MAE, RMSE, SMAPE, PoissonLoss, QuantileLoss\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "from pytorch_forecasting.utils import profile\n",
    "\n",
    "warnings.simplefilter(\"error\", category=SettingWithCopyWarning)\n",
    "\n",
    "# ================================================================================\n",
    "\n",
    "data = get_stallion_data()\n",
    "\n",
    "data[\"month\"] = data.date.dt.month.astype(\"str\").astype(\"category\")\n",
    "data[\"log_volume\"] = np.log(data.volume + 1e-8)\n",
    "\n",
    "data[\"time_idx\"] = data[\"date\"].dt.year * 12 + data[\"date\"].dt.month\n",
    "data[\"time_idx\"] -= data[\"time_idx\"].min()\n",
    "data[\"avg_volume_by_sku\"] = data.groupby([\"time_idx\", \"sku\"], observed=True).volume.transform(\"mean\")\n",
    "data[\"avg_volume_by_agency\"] = data.groupby([\"time_idx\", \"agency\"], observed=True).volume.transform(\"mean\")\n",
    "# data = data[lambda x: (x.sku == data.iloc[0][\"sku\"]) & (x.agency == data.iloc[0][\"agency\"])]\n",
    "special_days = [\n",
    "    \"easter_day\",\n",
    "    \"good_friday\",\n",
    "    \"new_year\",\n",
    "    \"christmas\",\n",
    "    \"labor_day\",\n",
    "    \"independence_day\",\n",
    "    \"revolution_day_memorial\",\n",
    "    \"regional_games\",\n",
    "    \"fifa_u_17_world_cup\",\n",
    "    \"football_gold_cup\",\n",
    "    \"beer_capital\",\n",
    "    \"music_fest\",\n",
    "]\n",
    "data[special_days] = data[special_days].apply(lambda x: x.map({0: \"\", 1: x.name})).astype(\"category\")\n",
    "\n",
    "training_cutoff = data[\"time_idx\"].max() - 6\n",
    "max_encoder_length = 36\n",
    "max_prediction_length = 6\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"volume\",\n",
    "    group_ids=[\"agency\", \"sku\"],\n",
    "    min_encoder_length=max_encoder_length // 2,  # allow encoder lengths from 0 to max_prediction_length\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"agency\", \"sku\"],\n",
    "    static_reals=[\"avg_population_2017\", \"avg_yearly_household_income_2017\"],\n",
    "    time_varying_known_categoricals=[\"special_days\", \"month\"],\n",
    "    variable_groups={\"special_days\": special_days},  # group of categorical variables can be treated as one variable\n",
    "    time_varying_known_reals=[\"time_idx\", \"price_regular\", \"discount_in_percent\"],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\n",
    "        \"volume\",\n",
    "        \"log_volume\",\n",
    "        \"industry_volume\",\n",
    "        \"soda_volume\",\n",
    "        \"avg_max_temp\",\n",
    "        \"avg_volume_by_agency\",\n",
    "        \"avg_volume_by_sku\",\n",
    "    ],\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"agency\", \"sku\"], transformation=\"softplus\", center=False\n",
    "    ),  # use softplus with beta=1.0 and normalize by group\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")\n",
    "\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(training, data, predict=True, stop_randomization=True)\n",
    "batch_size = 64\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "\n",
    "# save datasets\n",
    "training.save(\"t raining.pkl\")\n",
    "validation.save(\"validation.pkl\")\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()\n",
    "logger = TensorBoardLogger(log_graph=True)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    accelerator=\"auto\",\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=30,\n",
    "    # val_check_interval=20,\n",
    "    # limit_val_batches=1,\n",
    "    # fast_dev_run=True,\n",
    "    logger=logger,\n",
    "    # profiler=True,\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    ")\n",
    "\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    output_size=7,\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10,\n",
    "    log_val_interval=1,\n",
    "    reduce_on_plateau_patience=3,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n",
    "\n",
    "# # find optimal learning rate\n",
    "# # remove logging and artificial epoch size\n",
    "# tft.hparams.log_interval = -1\n",
    "# tft.hparams.log_val_interval = -1\n",
    "# trainer.limit_train_batches = 1.0\n",
    "# # run learning rate finder\n",
    "# res = Tuner(trainer).lr_find(\n",
    "#     tft, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader, min_lr=1e-5, max_lr=1e2\n",
    "# )\n",
    "# print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "# fig = res.plot(show=True, suggest=True)\n",
    "# fig.show()\n",
    "# tft.hparams.learning_rate = res.suggestion()\n",
    "\n",
    "# trainer.fit(\n",
    "#     tft,\n",
    "#     train_dataloaders=train_dataloader,\n",
    "#     val_dataloaders=val_dataloader,\n",
    "# )\n",
    "\n",
    "# # make a prediction on entire validation set\n",
    "# preds, index = tft.predict(val_dataloader, return_index=True, fast_dev_run=True)\n",
    "\n",
    "\n",
    "# tune\n",
    "study = optimize_hyperparameters(\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    model_path=\"optuna_test\",\n",
    "    n_trials=200,\n",
    "    max_epochs=50,\n",
    "    gradient_clip_val_range=(0.01, 1.0),\n",
    "    hidden_size_range=(8, 128),\n",
    "    hidden_continuous_size_range=(8, 128),\n",
    "    attention_head_size_range=(1, 4),\n",
    "    learning_rate_range=(0.001, 0.1),\n",
    "    dropout_range=(0.1, 0.3),\n",
    "    trainer_kwargs=dict(limit_train_batches=30),\n",
    "    reduce_on_plateau_patience=4,\n",
    "    use_learning_rate_finder=False,\n",
    ")\n",
    "with open(\"test_study.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study, fout)\n",
    "\n",
    "\n",
    "# profile speed\n",
    "# profile(\n",
    "#     trainer.fit,\n",
    "#     profile_fname=\"profile.prof\",\n",
    "#     model=tft,\n",
    "#     period=0.001,\n",
    "#     filter=\"pytorch_forecasting\",\n",
    "#     train_dataloaders=train_dataloader,\n",
    "#     val_dataloaders=val_dataloader,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이걸 읽어서 내걸로 만들어야한다니...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch_NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
